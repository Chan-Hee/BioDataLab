{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taewan/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def random_sample(xdata, ydata):\n",
    "    data_x = xdata.iloc[gene_idx,3:-1]\n",
    "    data_x = data_x.as_matrix()\n",
    "    data_x = data_x.transpose()\n",
    "    train_x = data_x[indexs[2000:],:]\n",
    "    test_x = data_x[indexs[:2000],:]\n",
    "    data_y = ydata[1:, 1:]    # eliminate heading, string data\n",
    "    # One-Hot-Encoding\n",
    "    data_y = data_y.flatten()\n",
    "    data_y = pd.get_dummies(data_y)\n",
    "    train_y = data_y.loc[indexs[2000:],:]\n",
    "    test_y = data_y.loc[indexs[:2000],:]\n",
    "    \n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def five_fold(data, num):\n",
    "    test_set = data[indexs[num*2000:(num+1)*2000]]\n",
    "    train_set = data[indexs[:num*2000]]\n",
    "    train_set = np.concatenate((train_set,data[indexs[(num+1)*2000:]] ), axis=0)\n",
    "    return train_set , test_set\n",
    "\n",
    "def cal_var(variances, per):\n",
    "    all_cnt = len(variances)\n",
    "    per = 100-per\n",
    "    per_idx = int(all_cnt*(per/100))\n",
    "    print(variances[per_idx])\n",
    "    return variances[per_idx]\n",
    "    \n",
    "#def random_five_fold(data, num, indexs):\n",
    "#    test_set = data[:, indexs[:2000]]\n",
    "#    train_set = data[ :, indexs[2000:]]\n",
    "#    #train_set = np.concatenate((train_set,data[(num+1)*2000:] ), axis=0)\n",
    "#    return train_set , test_set\n",
    "\n",
    "def top_of_variance(per, data_x):\n",
    "    ##data_x['variance']\n",
    "    ##calculate value  \n",
    "    data = data_x[data_x > per]\n",
    "    idx_list = data.index.tolist()\n",
    "    ##return index \n",
    "    return idx_list\n",
    "\n",
    "def set_train_three_layer(num,repeat, nodes, learning_rate):\n",
    "    tf.reset_default_graph()\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    train_a = 0\n",
    "    test_a = 0\n",
    "    X = tf.placeholder(tf.float32, [None, cnt_train])\n",
    "    Y = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "    W1 = tf.get_variable( shape= [cnt_train, nodes[0]], name='weight1' , initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([nodes[0]]), name='bias1')\n",
    "    layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    layer1 = tf.nn.dropout(layer1, keep_prob=keep_prob)\n",
    "\n",
    "    W2 = tf.get_variable(shape =[nodes[0], nodes[1]], name='weight2', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([nodes[1]]), name='bias2')\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "    layer2 = tf.nn.dropout(layer2 , keep_prob=keep_prob)\n",
    "\n",
    "    W3 = tf.get_variable(shape= [nodes[1], nodes[2]], name='weight3',initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([nodes[2]]), name='bias3')\n",
    "    layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "    layer3 = tf.nn.dropout(layer3, keep_prob=keep_prob)\n",
    "\n",
    "    W4 = tf.get_variable(shape=[nodes[2], 2], name='weight4',initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([2]), name='bias4')\n",
    "    hypothesis = tf.matmul(layer3, W4) + b4\n",
    "    \n",
    "\n",
    "\n",
    "    # cost/loss function\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "    # Accuracy computation\n",
    "    # True if hypothesis>0.5 else False\n",
    "\n",
    "    predicted = tf.argmax(hypothesis,1)\n",
    "    correct_prediction = tf.equal(predicted,tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize TensorFlow variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for step in range(repeat):\n",
    "            sess.run(train, feed_dict={X: train_x, Y: train_y , keep_prob : 0.7})\n",
    "            if step == repeat-1:\n",
    "                ####Train Accuracy report####\n",
    "                h, c, train_a = sess.run([hypothesis, predicted, accuracy],feed_dict={X: train_x, Y: train_y, keep_prob :0.7})\n",
    "                print(\"\\nTrain Accuracy: \", train_a)\n",
    "            if step % 20 == 0 :\n",
    "                h,c, p,train_a = sess.run([hypothesis, cost ,predicted, accuracy],feed_dict={X: train_x, Y: train_y, keep_prob :0.7})\n",
    "                print(\"\\nCurrent Accuracy : \", train_a , \"cost : \", c , \"Current Step : \", step)\n",
    "                if train_a > 0.95 :\n",
    "                    break\n",
    "        ######Accuracy Report#####\n",
    "        h, c, test_a = sess.run([hypothesis, predicted, accuracy],feed_dict={X: test_x, Y: test_y, keep_prob :1.0})    \n",
    "        print(\"\\nTest Accuracy: \", test_a)\n",
    "    \n",
    "    return train_a, test_a\n",
    "        \n",
    "def set_train_four_layer(num ,repeat, nodes, learning_rate):\n",
    "    tf.reset_default_graph()\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    train_a = 0\n",
    "    test_a = 0\n",
    "    X = tf.placeholder(tf.float32, [None, cnt_train])\n",
    "    Y = tf.placeholder(tf.float32, [None, 2])\n",
    "    \n",
    "    W1 = tf.get_variable( shape= [cnt_train, nodes[0]], name='Weight1' , initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([nodes[0]]), name='Bias1')\n",
    "    layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    layer1 = tf.nn.dropout(layer1, keep_prob=keep_prob)\n",
    "    \n",
    "    W2 = tf.get_variable(shape =[nodes[0], nodes[1]], name='Weight2', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([nodes[1]]), name='Bias2')\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "    layer2 = tf.nn.dropout(layer2, keep_prob=keep_prob)\n",
    "    \n",
    "    W3 = tf.get_variable(shape= [nodes[1], nodes[2]], name='Weight3',initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([nodes[2]]), name='Bias3')\n",
    "    layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "    layer3 = tf.nn.dropout(layer3, keep_prob=keep_prob)\n",
    "\n",
    "    W4 = tf.get_variable(shape = [nodes[2], nodes[3]] , name='Weight4' , initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([nodes[3]]), name='Bias4')\n",
    "    layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "    layer4 = tf.nn.dropout(layer4, keep_prob=keep_prob)\n",
    "\n",
    "    W5 = tf.get_variable(shape = [nodes[3], 2],name='Weight5',initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([2]), name='Bias5')\n",
    "    hypothesis = tf.matmul(layer4, W5) + b5\n",
    "\n",
    "    # cost/loss function\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "    # Accuracy computation\n",
    "\n",
    "\n",
    "    predicted = tf.argmax(hypothesis,1)\n",
    "    correct_prediction = tf.equal(predicted,tf.argmax(Y,1))\n",
    "\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize TensorFlow variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for step in range(repeat):\n",
    "            sess.run(train, feed_dict={X: train_x, Y: train_y, keep_prob : 0.7})\n",
    "            if step == repeat-1:\n",
    "                ####Train Accuracy report####\n",
    "                h, c, train_a = sess.run([hypothesis, predicted, accuracy],feed_dict={X: train_x, Y: train_y, keep_prob :0.7})\n",
    "                print(\"\\nTrain Accuracy: \", train_a)\n",
    "            if step % 20 == 0 : \n",
    "                h, c, p,train_a = sess.run([hypothesis, cost ,predicted, accuracy],feed_dict={X: train_x, Y: train_y, keep_prob :0.7})\n",
    "                print(\"\\nCurrent Accuracy : \", train_a , \"Cost : \",c , \"Current Step : \", step)\n",
    "                if train_a > 0.95 :\n",
    "                    break\n",
    "\n",
    "        ######Accuracy Report#####\n",
    "        h, c, test_a = sess.run([hypothesis, predicted, accuracy],feed_dict={X: test_x, Y: test_y, keep_prob :1.0})    \n",
    "        print(\"\\nTest Accuracy: \", test_a)\n",
    "    \n",
    "    return train_a, test_a\n",
    "\n",
    "####Read data####\n",
    "#x_filename = input(\"Insert X dataset directory and name  : \")\n",
    "#y_filename = input(\"Insert Y dataset directory and name : \")\n",
    "x_filename = '/home/tjahn/Data/DNN10000/DNN10000.csv'\n",
    "xdata = pd.read_csv(x_filename)\n",
    "ydata = np.genfromtxt('/home/tjahn/Data/DNN10000/CancerResult.csv', delimiter=\",\")\n",
    "#conf_filename = input(\"Insert configure file directory and name : \")\n",
    "conf_directory = '/home/tjahn/Git/Data/'\n",
    "conf_filename = 'input/relu_test_ps7.csv'\n",
    "conf = pd.read_csv(conf_directory+conf_filename)\n",
    "\n",
    "\n",
    "# train_x, test_x, train_y, test_y = random_sample(xdata, ydata)\n",
    "#variance를 한번 뽑아야한다. \n",
    "####################### Variance를 구하기 위해 자른다. ##############################\n",
    "####################### 데이터에서 정보 부분만큼의 길이를 이용해서 random한 index를 만든다. ###############################\n",
    "\n",
    "datas_x = xdata.iloc[:,3:-1]\n",
    "indexs = list(range(len(datas_x.iloc[1])))\n",
    "random.shuffle(indexs)\n",
    "train_datas = datas_x.iloc[:,indexs[2000:]]\n",
    "\n",
    "####################### 랜덤한 정보들 중 필요한 정보만을 취한다. ############################\n",
    "variances = train_datas.var(axis = 1)\n",
    "################ 여기서 index를 뽑아서 , gene 이름을 가지고 있어야 할 것 같다. ##########################\n",
    "####################### 분산 값을 가지고 있다. ###############################\n",
    "sorted_var = np.sort(variances)\n",
    "sorted_var = sorted_var[::-1]\n",
    "per = 1   #gene percent of variances \n",
    "idx = int((per/100)*len(sorted_var))\n",
    "####################### 분산의 양에 따라 뽑아낸다. ########################\n",
    "\n",
    "# print(sorted_var[idx])\n",
    "gene_idx = top_of_variance(sorted_var[idx], variances)\n",
    "xdata = xdata.iloc[:,3:-1] \n",
    "xdata = xdata.iloc[gene_idx]\n",
    "####################### 분산에 의해 뽑힌 Gene idx 들 ######################\n",
    "######################### Train set에 의해 구해진 variance를 기준으로 자른 index들 ##########################\n",
    "######################## 결정 된 gene 정보와 결정 된 shuffle index 2가지를 이용해서 문제를 해결하자 #########################\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for i in range(len(conf)):\n",
    "    repeat, layer, node , learning_rate, gene = conf.iloc[i]\n",
    "    nodes = list(map(int , node.split(\" \")))\n",
    "    train_accs_conf = []\n",
    "    test_accs_conf = []\n",
    "    for j in range(5):\n",
    "        data_x = xdata\n",
    "        data_x = data_x.as_matrix()\n",
    "        data_y = ydata[1:, 1:]    # eliminate heading, string data\n",
    "        data_x = data_x.transpose()\n",
    "        ###5-fold data \n",
    "        ####Data Processing - divide train and test set####\n",
    "        ####5-fold code  is needed ####\n",
    "        train_x, test_x = five_fold(data_x,j)\n",
    "        train_y, test_y = five_fold(data_y,j)\n",
    "        cnt_train = len(train_x[1, :])\n",
    "        if(conf.iloc[i]['layer'] == 3):\n",
    "            train_acc , test_acc = (set_train_three_layer(i,repeat, nodes, learning_rate))\n",
    "            train_accs_conf.append(train_acc)\n",
    "            test_accs_conf.append(test_acc)\n",
    "        elif(conf.iloc[i]['layer']== 4):\n",
    "            train_acc , test_acc = (set_train_four_layer(i,repeat, nodes, learning_rate))\n",
    "            train_accs_conf.append(train_acc)\n",
    "            test_accs_conf.append(test_acc)\n",
    "    train_accs.append(train_accs_conf)\n",
    "    test_accs.append(test_accs_conf)\n",
    "\n",
    "\n",
    "train_accs = pd.DataFrame(data=train_accs , \n",
    "                          index = list(range(len(conf))) , \n",
    "                          columns = [\"tr-fold-1\",\"tr-fold-2\",\"tr-fold-3\",\"tr-fold-4\",\"tr-fold-5\"])\n",
    "test_accs = pd.DataFrame(data=test_accs , \n",
    "                          index = list(range(len(conf))) , \n",
    "                          columns = [\"te-fold-1\",\"te-fold-2\",\"te-fold-3\",\"te-fold-4\",\"te-fold-5\"])\n",
    "\n",
    "accuracies = pd.concat([train_accs, test_accs], axis=1)\n",
    "conf = pd.concat([conf, accuracies] , axis = 1)\n",
    "conf.to_csv( conf_directory+'output'+conf_filename[5:-4] +'_result.csv' , sep= ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.758  4.796  5.089 ...,  3.981  5.527  5.772]\n",
      " [ 5.589  5.796  6.963 ...,  8.403  9.142  6.868]\n",
      " [ 5.436  0.681  4.296 ...,  1.548  5.573  3.37 ]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 0.81   0.495  1.039 ...,  2.139  0.79   4.171]\n",
      " [ 2.516  2.413  2.347 ...,  2.106  2.687  2.613]\n",
      " [ 5.216  6.223  4.917 ...,  6.473  9.099  7.067]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 5.758  4.796  5.089 ...,  3.981  5.527  5.772]\n",
      " [ 5.589  5.796  6.963 ...,  8.403  9.142  6.868]\n",
      " [ 5.436  0.681  4.296 ...,  1.548  5.573  3.37 ]\n",
      " ..., \n",
      " [ 5.164  6.681  4.395 ...,  4.266  8.123  8.022]\n",
      " [ 5.352  5.984  4.883 ...,  5.67   5.497  6.285]\n",
      " [ 4.081  4.461  4.585 ...,  1.385  6.447  5.659]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[  3.77200000e+00   4.66500000e+00   5.51900000e+00 ...,   1.27260000e+01\n",
      "    9.46800000e+00   5.41100000e+00]\n",
      " [  4.37100000e+00   4.81800000e+00   3.87900000e+00 ...,   9.80500000e+00\n",
      "    7.35900000e+00   5.49300000e+00]\n",
      " [ -1.92300000e+00  -1.87700000e+00  -6.28000000e-01 ...,  -1.14100000e+00\n",
      "   -8.05000000e-01  -9.64000000e-01]\n",
      " ..., \n",
      " [  1.78800000e+00   2.29900000e+00   8.18400000e+00 ...,   4.85500000e+00\n",
      "    4.09400000e+00   7.74200000e+00]\n",
      " [  2.46500000e+00   2.68600000e+00   2.76600000e+00 ...,   1.10000000e-02\n",
      "    1.93100000e+00   2.47300000e+00]\n",
      " [  5.84300000e+00   4.56700000e+00   8.95800000e+00 ...,   1.43560000e+01\n",
      "    9.49900000e+00   4.64200000e+00]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 2.652  2.334  2.361 ...,  3.147  3.193  2.764]\n",
      " [ 4.541  4.989  4.946 ...,  7.066  6.931  6.552]\n",
      " [ 6.509  6.37   6.043 ...,  9.802  9.437  6.955]\n",
      " ..., \n",
      " [ 3.339  2.622  2.913 ...,  2.319  2.667  3.288]\n",
      " [ 1.233 -0.564  1.473 ...,  2.966  2.372  1.326]\n",
      " [ 4.499  4.985  2.779 ...,  7.036  6.397  6.902]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 2.559  2.575  2.529 ...,  2.488  2.98   2.76 ]\n",
      " [ 2.075  2.113  2.018 ...,  2.084  2.194  2.303]\n",
      " [ 7.571  6.566  6.242 ...,  2.397  5.779  7.03 ]\n",
      " ..., \n",
      " [ 2.237  2.439  2.23  ...,  1.639  2.343  2.734]\n",
      " [ 4.668  5.008  7.245 ...,  4.318  1.523  8.254]\n",
      " [ 1.683  1.565  1.441 ...,  2.032  2.019  1.966]]\n",
      "[[ 5.758  4.796  5.089 ...,  3.981  5.527  5.772]\n",
      " [ 5.589  5.796  6.963 ...,  8.403  9.142  6.868]\n",
      " [ 5.436  0.681  4.296 ...,  1.548  5.573  3.37 ]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 0.81   0.495  1.039 ...,  2.139  0.79   4.171]\n",
      " [ 2.516  2.413  2.347 ...,  2.106  2.687  2.613]\n",
      " [ 5.216  6.223  4.917 ...,  6.473  9.099  7.067]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 5.758  4.796  5.089 ...,  3.981  5.527  5.772]\n",
      " [ 5.589  5.796  6.963 ...,  8.403  9.142  6.868]\n",
      " [ 5.436  0.681  4.296 ...,  1.548  5.573  3.37 ]\n",
      " ..., \n",
      " [ 5.164  6.681  4.395 ...,  4.266  8.123  8.022]\n",
      " [ 5.352  5.984  4.883 ...,  5.67   5.497  6.285]\n",
      " [ 4.081  4.461  4.585 ...,  1.385  6.447  5.659]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[  3.77200000e+00   4.66500000e+00   5.51900000e+00 ...,   1.27260000e+01\n",
      "    9.46800000e+00   5.41100000e+00]\n",
      " [  4.37100000e+00   4.81800000e+00   3.87900000e+00 ...,   9.80500000e+00\n",
      "    7.35900000e+00   5.49300000e+00]\n",
      " [ -1.92300000e+00  -1.87700000e+00  -6.28000000e-01 ...,  -1.14100000e+00\n",
      "   -8.05000000e-01  -9.64000000e-01]\n",
      " ..., \n",
      " [  1.78800000e+00   2.29900000e+00   8.18400000e+00 ...,   4.85500000e+00\n",
      "    4.09400000e+00   7.74200000e+00]\n",
      " [  2.46500000e+00   2.68600000e+00   2.76600000e+00 ...,   1.10000000e-02\n",
      "    1.93100000e+00   2.47300000e+00]\n",
      " [  5.84300000e+00   4.56700000e+00   8.95800000e+00 ...,   1.43560000e+01\n",
      "    9.49900000e+00   4.64200000e+00]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 2.652  2.334  2.361 ...,  3.147  3.193  2.764]\n",
      " [ 4.541  4.989  4.946 ...,  7.066  6.931  6.552]\n",
      " [ 6.509  6.37   6.043 ...,  9.802  9.437  6.955]\n",
      " ..., \n",
      " [ 3.339  2.622  2.913 ...,  2.319  2.667  3.288]\n",
      " [ 1.233 -0.564  1.473 ...,  2.966  2.372  1.326]\n",
      " [ 4.499  4.985  2.779 ...,  7.036  6.397  6.902]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 2.559  2.575  2.529 ...,  2.488  2.98   2.76 ]\n",
      " [ 2.075  2.113  2.018 ...,  2.084  2.194  2.303]\n",
      " [ 7.571  6.566  6.242 ...,  2.397  5.779  7.03 ]\n",
      " ..., \n",
      " [ 2.237  2.439  2.23  ...,  1.639  2.343  2.734]\n",
      " [ 4.668  5.008  7.245 ...,  4.318  1.523  8.254]\n",
      " [ 1.683  1.565  1.441 ...,  2.032  2.019  1.966]]\n",
      "[[ 5.758  4.796  5.089 ...,  3.981  5.527  5.772]\n",
      " [ 5.589  5.796  6.963 ...,  8.403  9.142  6.868]\n",
      " [ 5.436  0.681  4.296 ...,  1.548  5.573  3.37 ]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 0.81   0.495  1.039 ...,  2.139  0.79   4.171]\n",
      " [ 2.516  2.413  2.347 ...,  2.106  2.687  2.613]\n",
      " [ 5.216  6.223  4.917 ...,  6.473  9.099  7.067]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 5.758  4.796  5.089 ...,  3.981  5.527  5.772]\n",
      " [ 5.589  5.796  6.963 ...,  8.403  9.142  6.868]\n",
      " [ 5.436  0.681  4.296 ...,  1.548  5.573  3.37 ]\n",
      " ..., \n",
      " [ 5.164  6.681  4.395 ...,  4.266  8.123  8.022]\n",
      " [ 5.352  5.984  4.883 ...,  5.67   5.497  6.285]\n",
      " [ 4.081  4.461  4.585 ...,  1.385  6.447  5.659]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[  3.77200000e+00   4.66500000e+00   5.51900000e+00 ...,   1.27260000e+01\n",
      "    9.46800000e+00   5.41100000e+00]\n",
      " [  4.37100000e+00   4.81800000e+00   3.87900000e+00 ...,   9.80500000e+00\n",
      "    7.35900000e+00   5.49300000e+00]\n",
      " [ -1.92300000e+00  -1.87700000e+00  -6.28000000e-01 ...,  -1.14100000e+00\n",
      "   -8.05000000e-01  -9.64000000e-01]\n",
      " ..., \n",
      " [  1.78800000e+00   2.29900000e+00   8.18400000e+00 ...,   4.85500000e+00\n",
      "    4.09400000e+00   7.74200000e+00]\n",
      " [  2.46500000e+00   2.68600000e+00   2.76600000e+00 ...,   1.10000000e-02\n",
      "    1.93100000e+00   2.47300000e+00]\n",
      " [  5.84300000e+00   4.56700000e+00   8.95800000e+00 ...,   1.43560000e+01\n",
      "    9.49900000e+00   4.64200000e+00]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 2.652  2.334  2.361 ...,  3.147  3.193  2.764]\n",
      " [ 4.541  4.989  4.946 ...,  7.066  6.931  6.552]\n",
      " [ 6.509  6.37   6.043 ...,  9.802  9.437  6.955]\n",
      " ..., \n",
      " [ 3.339  2.622  2.913 ...,  2.319  2.667  3.288]\n",
      " [ 1.233 -0.564  1.473 ...,  2.966  2.372  1.326]\n",
      " [ 4.499  4.985  2.779 ...,  7.036  6.397  6.902]]\n",
      "[[ 6.71   3.996  5.331 ...,  8.366  7.652  5.999]\n",
      " [ 6.422  5.323  4.973 ..., -0.621  5.066  5.757]\n",
      " [ 2.455  2.442  2.51  ...,  2.532  2.211  2.712]\n",
      " ..., \n",
      " [ 4.354  2.122  2.763 ...,  7.361  5.577  7.58 ]\n",
      " [ 5.211  4.797  7.596 ...,  9.844  4.515  7.66 ]\n",
      " [ 2.315  2.556  2.898 ...,  2.912  2.497  2.458]] [[ 2.559  2.575  2.529 ...,  2.488  2.98   2.76 ]\n",
      " [ 2.075  2.113  2.018 ...,  2.084  2.194  2.303]\n",
      " [ 7.571  6.566  6.242 ...,  2.397  5.779  7.03 ]\n",
      " ..., \n",
      " [ 2.237  2.439  2.23  ...,  1.639  2.343  2.734]\n",
      " [ 4.668  5.008  7.245 ...,  4.318  1.523  8.254]\n",
      " [ 1.683  1.565  1.441 ...,  2.032  2.019  1.966]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
