{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Import Modules####\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "\n",
    "####Set Random Seed ####\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "def random_sample_fivefold(xdata, ydata , num):\n",
    "    ####################### Variance를 구하기 위해 자른다. ##############################\n",
    "    datas_x = xdata.iloc[:,3:-1]\n",
    "    ####How to random sampling?####\n",
    "    indexs = list(range(len(datas_x.iloc[1])))\n",
    "    # print(indexs)\n",
    "    random.shuffle(indexs)\n",
    "    train_datas = pd.concat([datas_x.iloc[:,indexs[:2000*num]], datas_x.iloc[:,indexs[2000*(num+1):]]])\n",
    "    print(train_datas)\n",
    "    print(len(train_datas))\n",
    "    # train_datas = datas_x.iloc[:,indexs[2000:]]\n",
    "    variances = train_datas.var(axis = 1)\n",
    "    sorted_var = np.sort(variances)\n",
    "    sorted_var = sorted_var[::-1]\n",
    "    per = 1   #gene percent of variances \n",
    "    idx = int((per/100)*len(sorted_var))\n",
    "    # print(sorted_var[idx])\n",
    "    gene_idx = top_of_variance(sorted_var[idx], variances)\n",
    "    ######################### Train set에 의해 구해진 variance를 기준으로 자른 index들 ##########################3\n",
    "    \n",
    "    data_x = xdata.iloc[gene_idx,3:-1]\n",
    "    data_x = data_x.as_matrix()\n",
    "    data_x = data_x.transpose()\n",
    "    train_set = data_x[indexs[:2000*num]]\n",
    "    train_x = np.concatenate((train_set,data_x[indexs[(num+1)*2000:]]), axis=0)\n",
    "    test_x = data_x[indexs[2000*num:2000*(num+1)],:]\n",
    "    data_y = ydata[1:, 1:]    # eliminate heading, string data\n",
    "    # One-Hot-Encoding\n",
    "    data_y = data_y.flatten()\n",
    "    data_y = pd.get_dummies(data_y)\n",
    "    data_y = data_y.as_matrix()\n",
    "    train_set = data_y[indexs[:2000*num]]\n",
    "    train_y = np.concatenate((train_set,data_y[indexs[(num+1)*2000:]]), axis=0)\n",
    "    test_y = data_y[indexs[2000*num:2000*(num+1)],:]\n",
    "\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def random_sample(xdata, ydata):\n",
    "    ####################### Variance를 구하기 위해 자른다. ##############################\n",
    "    datas_x = xdata.iloc[:,3:-1]\n",
    "    ####How to random sampling?####\n",
    "    indexs = list(range(len(datas_x.iloc[1])))\n",
    "    # print(indexs)\n",
    "    random.shuffle(indexs)\n",
    "    train_datas = datas_x.iloc[:,indexs[2000:]]\n",
    "    variances = train_datas.var(axis = 1)\n",
    "    sorted_var = np.sort(variances)\n",
    "    sorted_var = sorted_var[::-1]\n",
    "    per = 1   #gene percent of variances \n",
    "    idx = int((per/100)*len(sorted_var))\n",
    "    # print(sorted_var[idx])\n",
    "    gene_idx = top_of_variance(sorted_var[idx], variances)\n",
    "    ######################### Train set에 의해 구해진 variance를 기준으로 자른 index들 ##########################3\n",
    "    \n",
    "    data_x = xdata.iloc[gene_idx,3:-1]\n",
    "    data_x = data_x.as_matrix()\n",
    "    data_x = data_x.transpose()\n",
    "    train_x = data_x[indexs[2000:],:]\n",
    "    test_x = data_x[indexs[:2000],:]\n",
    "    data_y = ydata[1:, 1:]    # eliminate heading, string data\n",
    "    # One-Hot-Encoding\n",
    "    data_y = data_y.flatten()\n",
    "    data_y = pd.get_dummies(data_y)\n",
    "    train_y = data_y.loc[indexs[2000:],:]\n",
    "    test_y = data_y.loc[indexs[:2000],:]\n",
    "    \n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def cal_var(variances, per):\n",
    "    all_cnt = len(variances)\n",
    "    per = 100-per\n",
    "    per_idx = int(all_cnt*(per/100))\n",
    "    print(variances[per_idx])\n",
    "    return variances[per_idx]\n",
    "    \n",
    "#def random_five_fold(data, num, indexs):\n",
    "#    test_set = data[:, indexs[:2000]]\n",
    "#    train_set = data[ :, indexs[2000:]]\n",
    "#    #train_set = np.concatenate((train_set,data[(num+1)*2000:] ), axis=0)\n",
    "#    return train_set , test_set\n",
    "\n",
    "def top_of_variance(per, data_x):\n",
    "    ##data_x['variance']\n",
    "    ##calculate value  \n",
    "    data = data_x[data_x > per]\n",
    "    idx_list = data.index.tolist()\n",
    "    ##return index \n",
    "    return idx_list\n",
    "\n",
    "def set_train_three_layer(num,repeat, nodes, learning_rate):\n",
    "    tf.reset_default_graph()\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    train_a = 0\n",
    "    test_a = 0\n",
    "    X = tf.placeholder(tf.float32, [None, cnt_train])\n",
    "    Y = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "    W1 = tf.get_variable( shape= [cnt_train, nodes[0]], name='weight1' , initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([nodes[0]]), name='bias1')\n",
    "    layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    layer1 = tf.nn.dropout(layer1, keep_prob=keep_prob)\n",
    "\n",
    "    W2 = tf.get_variable(shape =[nodes[0], nodes[1]], name='weight2', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([nodes[1]]), name='bias2')\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "    layer2 = tf.nn.dropout(layer2 , keep_prob=keep_prob)\n",
    "\n",
    "    W3 = tf.get_variable(shape= [nodes[1], nodes[2]], name='weight3',initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([nodes[2]]), name='bias3')\n",
    "    layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "    layer3 = tf.nn.dropout(layer3, keep_prob=keep_prob)\n",
    "\n",
    "    W4 = tf.get_variable(shape=[nodes[2], 2], name='weight4',initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([2]), name='bias4')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "    \n",
    "\n",
    "\n",
    "    # cost/loss function\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "    # Accuracy computation\n",
    "    # True if hypothesis>0.5 else False\n",
    "\n",
    "    predicted = tf.argmax(hypothesis,1)\n",
    "    correct_prediction = tf.equal(predicted,tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize TensorFlow variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for step in range(repeat):\n",
    "            sess.run(train, feed_dict={X: train_x, Y: train_y , keep_prob : 0.7})\n",
    "            if step == repeat-1:\n",
    "                ####Train Accuracy report####\n",
    "                h, c, train_a = sess.run([hypothesis, predicted, accuracy],feed_dict={X: train_x, Y: train_y, keep_prob :0.7})\n",
    "                print(\"\\nTrain Accuracy: \", train_a)\n",
    "            if step % 20 == 0 :\n",
    "                h,c, p,train_a = sess.run([hypothesis, cost ,predicted, accuracy],feed_dict={X: train_x, Y: train_y, keep_prob :0.7})\n",
    "                print(\"\\nCurrent Accuracy : \", train_a , \"cost : \", c , \"Current Step : \", step)\n",
    "                if train_a > 0.95 :\n",
    "                    break\n",
    "        ######Accuracy Report#####\n",
    "        h, c, test_a = sess.run([hypothesis, predicted, accuracy],feed_dict={X: test_x, Y: test_y, keep_prob :1.0})    \n",
    "        print(\"\\nTest Accuracy: \", test_a)\n",
    "    \n",
    "    return train_a, test_a\n",
    "        \n",
    "def set_train_four_layer(num ,repeat, nodes, learning_rate):\n",
    "    tf.reset_default_graph()\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    train_a = 0\n",
    "    test_a = 0\n",
    "    X = tf.placeholder(tf.float32, [None, cnt_train])\n",
    "    Y = tf.placeholder(tf.float32, [None, 2])\n",
    "    \n",
    "    W1 = tf.get_variable( shape= [cnt_train, nodes[0]], name='Weight1' , initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([nodes[0]]), name='Bias1')\n",
    "    layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    layer1 = tf.nn.dropout(layer1, keep_prob=keep_prob)\n",
    "    \n",
    "    W2 = tf.get_variable(shape =[nodes[0], nodes[1]], name='Weight2', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([nodes[1]]), name='Bias2')\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "    layer2 = tf.nn.dropout(layer2, keep_prob=keep_prob)\n",
    "    \n",
    "    W3 = tf.get_variable(shape= [nodes[1], nodes[2]], name='Weight3',initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([nodes[2]]), name='Bias3')\n",
    "    layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "    layer3 = tf.nn.dropout(layer3, keep_prob=keep_prob)\n",
    "\n",
    "    W4 = tf.get_variable(shape = [nodes[2], nodes[3]] , name='Weight4' , initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([nodes[3]]), name='Bias4')\n",
    "    layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "    layer4 = tf.nn.dropout(layer4, keep_prob=keep_prob)\n",
    "\n",
    "    W5 = tf.get_variable(shape = [nodes[3], 2],name='Weight5',initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([2]), name='Bias5')\n",
    "    hypothesis = tf.matmul(layer4, W5) + b5\n",
    "\n",
    "    # cost/loss function\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "    # Accuracy computation\n",
    "\n",
    "\n",
    "    predicted = tf.argmax(hypothesis,1)\n",
    "    correct_prediction = tf.equal(predicted,tf.argmax(Y,1))\n",
    "\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize TensorFlow variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for step in range(repeat):\n",
    "            sess.run(train, feed_dict={X: train_x, Y: train_y, keep_prob : 0.7})\n",
    "            if step == repeat-1:\n",
    "                ####Train Accuracy report####\n",
    "                h, c, train_a = sess.run([hypothesis, predicted, accuracy],feed_dict={X: train_x, Y: train_y, keep_prob :0.7})\n",
    "                print(\"\\nTrain Accuracy: \", train_a)\n",
    "            if step % 20 == 0 : \n",
    "                h, c, p,train_a = sess.run([hypothesis, cost ,predicted, accuracy],feed_dict={X: train_x, Y: train_y, keep_prob :0.7})\n",
    "                print(\"\\nCurrent Accuracy : \", train_a , \"Cost : \",c , \"Current Step : \", step)\n",
    "                if train_a > 0.95 :\n",
    "                    break\n",
    "\n",
    "        ######Accuracy Report#####\n",
    "        h, c, test_a = sess.run([hypothesis, predicted, accuracy],feed_dict={X: test_x, Y: test_y, keep_prob :1.0})    \n",
    "        print(\"\\nTest Accuracy: \", test_a)\n",
    "    \n",
    "    return train_a, test_a\n",
    "\n",
    "####Read data####\n",
    "#x_filename = input(\"Insert X dataset directory and name  : \")\n",
    "#y_filename = input(\"Insert Y dataset directory and name : \")\n",
    "x_filename = '/Users/taewan/Desktop/Ahn/DNN10000.csv'\n",
    "xdata = pd.read_csv(x_filename)\n",
    "ydata = np.genfromtxt('/Users/taewan/Desktop/CancerResult10000.csv', delimiter=\",\")\n",
    "#conf_filename = input(\"Insert configure file directory and name : \")\n",
    "conf_filename = '/Users/taewan/Desktop/Ahn/Git/BioDataLab/Data/input/relu_test_ps.csv'\n",
    "conf = pd.read_csv(conf_filename)\n",
    "\n",
    "train_x, test_x, train_y, test_y = random_sample(xdata, ydata)\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for i in range(len(conf)):\n",
    "    repeat, layer, node , learning_rate, gene = conf.iloc[i]\n",
    "    nodes = list(map(int , node.split(\" \")))\n",
    "\n",
    "    #print(train_y)\n",
    "    cnt_train = len(train_x[1, :])\n",
    "    if(conf.iloc[i]['layer'] == 3):\n",
    "        train_acc , test_acc = (set_train_three_layer(i,repeat, nodes, learning_rate))\n",
    "    elif(conf.iloc[i]['layer']== 4):\n",
    "        train_acc , test_acc = (set_train_four_layer(i,repeat, nodes, learning_rate))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "\n",
    "train_accs = pd.DataFrame(data=train_accs , \n",
    "                          index = list(range(len(conf))) , \n",
    "                          columns = [\"train_acc\"])\n",
    "test_accs = pd.DataFrame(data=test_accs , \n",
    "                          index = list(range(len(conf))) , \n",
    "                          columns = [\"test_acc\"])\n",
    "\n",
    "accuracies = pd.concat([train_accs, test_accs], axis=1)\n",
    "conf = pd.concat([conf, accuracies] , axis = 1)\n",
    "conf.to_csv(  conf_filename[:-4] +'_result.csv' , sep= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taewan/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current Accuracy :  0.486021 cost :  0.741877 Current Step :  0\n",
      "\n",
      "Current Accuracy :  0.547678 cost :  0.686811 Current Step :  20\n",
      "\n",
      "Current Accuracy :  0.546306 cost :  0.677094 Current Step :  40\n",
      "\n",
      "Current Accuracy :  0.579381 cost :  0.661501 Current Step :  60\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-ba43f446c04c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcnt_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset_train_three_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset_train_four_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-cc2ab61dd63a>\u001b[0m in \u001b[0;36mset_train_three_layer\u001b[0;34m(num, repeat, nodes, learning_rate)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;31m####Train Accuracy report####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taewan/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taewan/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taewan/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taewan/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taewan/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8012\n",
      "2000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206.9\n",
      "[[0.575 1.2 1.163 ..., 0.408 0.23199999999999998 0.462]\n",
      " [0.545 1.004 1.0390000000000001 ..., 4.945 5.156000000000001 5.124]\n",
      " [0.743 0.963 0.909 ..., 1.694 1.381 1.527]\n",
      " ..., \n",
      " [-2.016 -1.087 -2.638 ..., 0.43799999999999994 0.486 0.368]\n",
      " [1.845 1.892 1.78 ..., 0.8859999999999999 1.139 0.807]\n",
      " [0.905 1.139 1.1079999999999999 ..., -0.408 -0.419 -0.397]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (8012, 733) for Tensor 'Placeholder:0', which has shape '(?, 4521)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-fd04f1d4ba2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#print(train_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset_train_three_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset_train_four_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-e6d6b58e965d>\u001b[0m in \u001b[0;36mset_train_three_layer\u001b[0;34m(num, repeat, nodes, learning_rate)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m####Train Accuracy report####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taewan/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taewan/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1101\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (8012, 733) for Tensor 'Placeholder:0', which has shape '(?, 4521)'"
     ]
    }
   ],
   "source": [
    "for i in range(len(conf)):\n",
    "\n",
    "    repeat, layer, node , learning_rate, gene = conf.iloc[i]\n",
    "    nodes = list(map(int , node.split(\" \")))\n",
    "\n",
    "    j = 0\n",
    "    cnt_train = len(xdata[:,-1])    \n",
    "    \n",
    "    # print(variances)\n",
    "\n",
    "    data_y = ydata[1:, 1:]    # eliminate heading, string data\n",
    "    # One-Hot-Encoding\n",
    "    data_y = data_y.flatten()\n",
    "    data_y = pd.get_dummies(data_y)\n",
    "\n",
    "\n",
    "    \n",
    "    ###############################Edit############################\n",
    "    print(data_x)\n",
    "    test_x = data_x[:,indexs[:2000]]\n",
    "    train_x = data_x[:,indexs[2000:]]\n",
    "    train_x = train_x.transpose()\n",
    "    test_x = test_x.transpose()\n",
    "#     print(data_y)\n",
    "\n",
    "#     train_y, test_y = random_five_fold(data_y,j,indexs)\n",
    "    ###############################Edit############################\n",
    "\n",
    "    #print(train_y)\n",
    "    if(conf.iloc[i]['layer'] == 3):\n",
    "        train_acc , test_acc = (set_train_three_layer(i,repeat, nodes, learning_rate))\n",
    "    elif(conf.iloc[i]['layer']== 4):\n",
    "        train_acc , test_acc = (set_train_four_layer(i,repeat, nodes, learning_rate))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "\n",
    "train_accs = pd.DataFrame(data=train_accs , \n",
    "                          index = list(range(len(conf))) , \n",
    "                          columns = [\"train_acc\"])\n",
    "test_accs = pd.DataFrame(data=test_accs , \n",
    "                          index = list(range(len(conf))) , \n",
    "                          columns = [\"test_acc\"])\n",
    "\n",
    "accuracies = pd.concat([train_accs, test_accs], axis=1)\n",
    "conf = pd.concat([conf, accuracies] , axis = 1)\n",
    "conf.to_csv('/home/tjahn/Git/Data/output/'+ conf_filename[:-4] +'.csv' , sep= ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
